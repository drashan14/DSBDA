{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r9HCLKkbHzrt",
    "outputId": "b7efdf13-aaba-4354-82ba-a2262144c0ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading https://files.pythonhosted.org/packages/b4/01/68fcc0d43daf4c6bdbc6b33cc3f77bda531c86b174cac56ef0ffdb96faab/PyPDF2-1.26.0.tar.gz (77kB)\n",
      "Building wheels for collected packages: PyPDF2\n",
      "  Building wheel for PyPDF2 (setup.py): started\n",
      "  Building wheel for PyPDF2 (setup.py): finished with status 'done'\n",
      "  Created wheel for PyPDF2: filename=PyPDF2-1.26.0-cp37-none-any.whl size=61091 sha256=0d4e38a2d84a2eb596926071afcfcc9af7ee84e4a96279e01fcc241d05e7915c\n",
      "  Stored in directory: C:\\Users\\HP\\AppData\\Local\\pip\\Cache\\wheels\\53\\84\\19\\35bc977c8bf5f0c23a8a011aa958acd4da4bbd7a229315c1b7\n",
      "Successfully built PyPDF2\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-1.26.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lezWe04nHzr0",
    "outputId": "8a2c4a73-2882-4437-c8d3-b0ea0239cc24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Downloading https://files.pythonhosted.org/packages/8b/a0/52729ce4aa026f31b74cc877be1d11e4ddeaa361dc7aebec148171644b33/python-docx-0.8.11.tar.gz (5.6MB)\n",
      "Requirement already satisfied: lxml>=2.3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-docx) (4.4.1)\n",
      "Building wheels for collected packages: python-docx\n",
      "  Building wheel for python-docx (setup.py): started\n",
      "  Building wheel for python-docx (setup.py): finished with status 'done'\n",
      "  Created wheel for python-docx: filename=python_docx-0.8.11-cp37-none-any.whl size=184607 sha256=4e01ad30c11a4aa6b83f7f4ed6a7d696a2fb4d55f2dfe2cef0f0f8cae90d846e\n",
      "  Stored in directory: C:\\Users\\HP\\AppData\\Local\\pip\\Cache\\wheels\\a6\\90\\f1\\a7cb70b38633ae04e7fb963b1c70f63fd6fc01c075b8230adc\n",
      "Successfully built python-docx\n",
      "Installing collected packages: python-docx\n",
      "Successfully installed python-docx-0.8.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rwpx3I0iHzr1",
    "outputId": "f10352e5-9286-4703-8b92-63ac40578db2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Welcome to Smallpdf\n",
      "Digital DocumentsŠAll In One Place\n",
      "Access Files Anytime, Anywhere \n",
      "Enhance Documents in One Click \n",
      "Collaborate With Others \n",
      "With the new Smallpdf experience, you can \n",
      "freely upload, organize, and share digital \n",
      "documents. When you enable the \n",
      "‚Storage™ \n",
      "option\n",
      "\n",
      "\n",
      "your computer, phone, or tablet. We™ll also \n",
      "\n",
      "Smallpdf Mobile App to our \n",
      "online portal\n",
      "\n",
      "you with an array of options to convert, \n",
      "compress, or modify it. \n",
      "Forget mundane administrative tasks. With \n",
      "Smallpdf, you can request e-signatures, send \n",
      "\n",
      "Smallpdf G Suite \n",
      "App for your entire organization. \n",
      "Ready to take document management to the next level? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing required modules \n",
    "import PyPDF2 \n",
    "    \n",
    "# creating a pdf file object \n",
    "pdfFileObj = open(r\"D:\\College\\TE\\SEM-2\\Practical\\DSBDA\\7\\sample1.pdf\", 'rb') \n",
    "    \n",
    "# creating a pdf reader object \n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj) \n",
    "    \n",
    "# printing number of pages in pdf file \n",
    "print(pdfReader.numPages) \n",
    "    \n",
    "# creating a page object \n",
    "pageObj = pdfReader.getPage(0) \n",
    "    \n",
    "# extracting text from page \n",
    "print(pageObj.extractText()) \n",
    "    \n",
    "# closing the pdf file object \n",
    "pdfFileObj.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9OqmTpY4Hzr3"
   },
   "outputs": [],
   "source": [
    "# import docx NOT python-docx\n",
    "import docx\n",
    "  \n",
    "# create an instance of a word document\n",
    "doc = docx.Document()\n",
    "  \n",
    "# add a heading of level 0 (largest heading)\n",
    "doc.add_heading('Heading for the document', 0)\n",
    "  \n",
    "# add a paragraph and store \n",
    "# the object in a variable\n",
    "doc_para = doc.add_paragraph('Your paragraph goes here, ')\n",
    "  \n",
    "# add a run i.e, style like \n",
    "# bold, italic, underline, etc.\n",
    "doc_para.add_run('hey there, bold here').bold = True\n",
    "doc_para.add_run(', and ')\n",
    "doc_para.add_run('these words are italic').italic = True\n",
    "  \n",
    "# add a page break to start a new page\n",
    "doc.add_page_break()\n",
    "  \n",
    "# add a heading of level 2\n",
    "doc.add_heading('Heading level 2', 2)\n",
    "  \n",
    "# pictures can also be added to our word document\n",
    "# width is optional\n",
    "doc.add_picture(r\"D:\\College\\TE\\SEM-2\\Practical\\DSBDA\\7\\index.jpg\")\n",
    "  \n",
    "# now save the document to a location\n",
    "doc.save('new_doc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lVbi3BHwHzr4",
    "outputId": "0028cd1a-6594-49b1-e8d0-73a5b3d6aadb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rdnoGOs1Hzr5",
    "outputId": "1681934c-db0a-4783-83ae-a0ff4b1cdfa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kdJULL3IHzr6",
    "outputId": "2ed680be-a0ad-4f5f-9ca5-4c991216ac4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The First sentence is about Python.', 'The Second: about Django.', 'You can learn Python,Django and Data Ananlysis here.']\n"
     ]
    }
   ],
   "source": [
    "#Sentence Tokenization\n",
    "\n",
    "sentence_data = \"The First sentence is about Python. The Second: about Django. You can learn Python,Django and Data Ananlysis here. \"\n",
    "nltk_tokens = nltk.sent_tokenize(sentence_data)\n",
    "print (nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lFSoVWabHzr7",
    "outputId": "45e190f9-fa6a-4c5f-a421-4cde12e7312d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wie geht es Ihnen?', 'Gut, danke.']\n"
     ]
    }
   ],
   "source": [
    "#Non English language Tokenization\n",
    "\n",
    "german_tokenizer = nltk.data.load('tokenizers/punkt/german.pickle')\n",
    "german_tokens=german_tokenizer.tokenize('Wie geht es Ihnen?  Gut, danke.')\n",
    "print(german_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZwNd_fYoHzr8",
    "outputId": "8aa8f23a-2b44-4d95-962d-d9a98741fbbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'originated', 'from', 'the', 'idea', 'that', 'there', 'are', 'readers', 'who', 'prefer', 'learning', 'new', 'skills', 'from', 'the', 'comforts', 'of', 'their', 'drawing', 'rooms']\n"
     ]
    }
   ],
   "source": [
    "#Word Tokenization\n",
    "\n",
    "word_data = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "print (nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G5NXxeG4Hzr9",
    "outputId": "21e90076-8ec2-474b-f1ac-3aa119f89b7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'is', 'a', 'boy', '.', 'She', 'is', 'a', 'girl']\n"
     ]
    }
   ],
   "source": [
    "#Word Tokenization\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "  \n",
    "#Dummy text\n",
    "txt = \"He is a boy. \"\\\n",
    "    \"She is a girl\"\n",
    "\n",
    "word_tokens = word_tokenize(txt)\n",
    "  \n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m1q_7aeTHzr-",
    "outputId": "53050fa9-b159-4b78-a8a9-472bcd8f5775"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Hello', 'NNP'),\n",
       " ('welcome', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('world', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('to', 'TO'),\n",
       " ('learn', 'VB'),\n",
       " ('Categorizing', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('POS', 'NNP'),\n",
       " ('Tagging', 'NNP'),\n",
       " ('with', 'IN'),\n",
       " ('NLTK', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Python', 'NNP')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Part of Speech (POS) tagging\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = word_tokenize(\"Hello welcome to the world of to learn Categorizing and POS Tagging with NLTK and Python\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-7JOBlU3Hzr_",
    "outputId": "1a37d5d5-4ac8-420a-de88-f3083bf76711"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rdRww8lTHzsA",
    "outputId": "97798dff-47f3-44b6-8c29-ac76391697b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P-v8o9aRHzsA",
    "outputId": "b12abe0d-dc63-455d-e834-62c33b52efaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized: ['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "Stop Words Removed: ['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "#Stopwords removal from sentence\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "example_sent = \"\"\"This is a sample sentence,\n",
    "                  showing off the stop words filtration.\"\"\"\n",
    " \n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "word_tokens = word_tokenize(example_sent)\n",
    " \n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    " \n",
    "filtered_sentence = []\n",
    " \n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    " \n",
    "print(\"Tokenized:\", word_tokens)\n",
    "print(\"Stop Words Removed:\", filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "wbAKWeX0HzsC"
   },
   "outputs": [],
   "source": [
    "#Stopwords from input file\n",
    "\n",
    "import io\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "# word_tokenize accepts\n",
    "# a string as an input, not a file.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "file1 = open(r\"D:\\College\\TE\\SEM-2\\Practical\\DSBDA\\7\\text.txt\")\n",
    " \n",
    "# Use this to read file content as a stream:\n",
    "line = file1.read()\n",
    "words = line.split()\n",
    "for r in words:\n",
    "    if not r in stop_words:\n",
    "        appendFile = open('filteredtext.txt','a')\n",
    "        appendFile.write(\" \"+r)\n",
    "        appendFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ooyFq1aJHzsC",
    "outputId": "7b8b00f0-40b9-4bc6-dd26-018792994a2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: It  Stem: It\n",
      "Actual: vijaying  Stem: vijay\n",
      "Actual: meeting  Stem: meet\n",
      "Actual: better  Stem: better\n",
      "Actual: vijayed  Stem: vijay\n",
      "Actual: vijays  Stem: vijay\n",
      "Actual: eats  Stem: eat\n",
      "Actual: skills  Stem: skill\n",
      "Actual: originated  Stem: origin\n",
      "Actual: from  Stem: from\n",
      "Actual: the  Stem: the\n",
      "Actual: idea  Stem: idea\n",
      "Actual: that  Stem: that\n",
      "Actual: there  Stem: there\n",
      "Actual: are  Stem: are\n",
      "Actual: readers  Stem: reader\n",
      "Actual: who  Stem: who\n",
      "Actual: prefer  Stem: prefer\n",
      "Actual: learning  Stem: learn\n",
      "Actual: new  Stem: new\n",
      "Actual: skills  Stem: skill\n",
      "Actual: from  Stem: from\n",
      "Actual: the  Stem: the\n",
      "Actual: comforts  Stem: comfort\n",
      "Actual: of  Stem: of\n",
      "Actual: their  Stem: their\n",
      "Actual: drawing  Stem: draw\n",
      "Actual: rooms  Stem: room\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "word_data = \"It vijaying meeting better vijayed vijays eats skills originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
    "# First Word tokenization\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "#Next find the roots of the word\n",
    "for w in nltk_tokens:\n",
    "       print(\"Actual: %s  Stem: %s\"  % (w,porter_stemmer.stem(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YSmXKl9MHzsD",
    "outputId": "bb510e80-fcbe-4748-887c-5bb4339cc916"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: It  Lemma: It\n",
      "Actual: studies  Lemma: study\n",
      "Actual: densely  Lemma: densely\n",
      "Actual: is  Lemma: is\n",
      "Actual: better  Lemma: better\n",
      "Actual: meeting  Lemma: meeting\n",
      "Actual: studying  Lemma: studying\n",
      "Actual: vijaying  Lemma: vijaying\n",
      "Actual: vijayed  Lemma: vijayed\n",
      "Actual: vijays  Lemma: vijays\n",
      "Actual: skills  Lemma: skill\n",
      "Actual: originated  Lemma: originated\n",
      "Actual: from  Lemma: from\n",
      "Actual: the  Lemma: the\n",
      "Actual: idea  Lemma: idea\n",
      "Actual: that  Lemma: that\n",
      "Actual: there  Lemma: there\n",
      "Actual: are  Lemma: are\n",
      "Actual: readers  Lemma: reader\n",
      "Actual: who  Lemma: who\n",
      "Actual: prefer  Lemma: prefer\n",
      "Actual: learning  Lemma: learning\n",
      "Actual: new  Lemma: new\n",
      "Actual: skills  Lemma: skill\n",
      "Actual: from  Lemma: from\n",
      "Actual: the  Lemma: the\n",
      "Actual: comforts  Lemma: comfort\n",
      "Actual: of  Lemma: of\n",
      "Actual: their  Lemma: their\n",
      "Actual: drawing  Lemma: drawing\n",
      "Actual: rooms  Lemma: room\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "word_data = \"It studies densely is better  meeting studying vijaying vijayed vijays skills originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "for w in nltk_tokens:\n",
    "        print(\"Actual: %s  Lemma: %s\"  % (w,wordnet_lemmatizer.lemmatize(w))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "J8SNEOlSHzsD"
   },
   "outputs": [],
   "source": [
    "#Expt.No.7 2nd Operation\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZJyLNJNEHzsD",
    "outputId": "9554d79c-d3d0-4b15-cf6a-defd5ae308b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'century', 'Data', 'of', 'for', '21st', 'job', 'learning', 'the', 'Science', 'is', 'key', 'Machine', 'science', 'best', 'data'}\n"
     ]
    }
   ],
   "source": [
    "first_sentence = \"Data Science is the best job of the 21st century\"\n",
    "second_sentence = \"Machine learning is the key for data science\"\n",
    "\n",
    "#split so each word have their own string\n",
    "first_sentence = first_sentence.split(\" \")\n",
    "second_sentence = second_sentence.split(\" \")#join them to remove common duplicate words\n",
    "total= set(first_sentence).union(set(second_sentence))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "7_Op7_EHHzsE",
    "outputId": "65c157ae-3735-454d-fc95-ff6f41f7da48"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>century</th>\n",
       "      <th>Data</th>\n",
       "      <th>of</th>\n",
       "      <th>for</th>\n",
       "      <th>21st</th>\n",
       "      <th>job</th>\n",
       "      <th>learning</th>\n",
       "      <th>the</th>\n",
       "      <th>Science</th>\n",
       "      <th>is</th>\n",
       "      <th>key</th>\n",
       "      <th>Machine</th>\n",
       "      <th>science</th>\n",
       "      <th>best</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   century  Data  of  for  21st  job  learning  the  Science  is  key  \\\n",
       "0        1     1   1    0     1    1         0    2        1   1    0   \n",
       "1        0     0   0    1     0    0         1    1        0   1    1   \n",
       "\n",
       "   Machine  science  best  data  \n",
       "0        0        0     1     0  \n",
       "1        1        1     0     1  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count the words\n",
    "\n",
    "wordDictA = dict.fromkeys(total, 0) \n",
    "wordDictB = dict.fromkeys(total, 0)\n",
    "for word in first_sentence:\n",
    "    wordDictA[word]+=1\n",
    "    \n",
    "for word in second_sentence:\n",
    "    wordDictB[word]+=1\n",
    "    \n",
    "pd.DataFrame([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "MxENejj3HzsE",
    "outputId": "6d95205b-1b6c-4703-e51e-a6e6d604c326"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>century</th>\n",
       "      <th>Data</th>\n",
       "      <th>of</th>\n",
       "      <th>for</th>\n",
       "      <th>21st</th>\n",
       "      <th>job</th>\n",
       "      <th>learning</th>\n",
       "      <th>the</th>\n",
       "      <th>Science</th>\n",
       "      <th>is</th>\n",
       "      <th>key</th>\n",
       "      <th>Machine</th>\n",
       "      <th>science</th>\n",
       "      <th>best</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   century  Data   of    for  21st  job  learning    the  Science     is  \\\n",
       "0      0.1   0.1  0.1  0.000   0.1  0.1     0.000  0.200      0.1  0.100   \n",
       "1      0.0   0.0  0.0  0.125   0.0  0.0     0.125  0.125      0.0  0.125   \n",
       "\n",
       "     key  Machine  science  best   data  \n",
       "0  0.000    0.000    0.000   0.1  0.000  \n",
       "1  0.125    0.125    0.125   0.0  0.125  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute Term Frequency(TF)\n",
    "\n",
    "def computeTF(wordDict, doc):\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(corpusCount)\n",
    "    return(tfDict)\n",
    "\n",
    "#running our sentences through the tf function:\n",
    "tfFirst = computeTF(wordDictA, first_sentence)\n",
    "tfSecond = computeTF(wordDictB, second_sentence)\n",
    "\n",
    "#Converting to dataframe for visualization\n",
    "pd.DataFrame([tfFirst, tfSecond])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "L6mbtNcrHzsF"
   },
   "outputs": [],
   "source": [
    "#Compute Inverse Document Frequency(IDF)\n",
    "\n",
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / (float(val) + 1))\n",
    "        \n",
    "    return(idfDict)\n",
    "\n",
    "#inputing our sentences in the log file\n",
    "idfs = computeIDF([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "QhZ1RQt3HzsF",
    "outputId": "2d1a71c7-21b3-4d73-f1fb-91b2874c92e3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>century</th>\n",
       "      <th>Data</th>\n",
       "      <th>of</th>\n",
       "      <th>for</th>\n",
       "      <th>21st</th>\n",
       "      <th>job</th>\n",
       "      <th>learning</th>\n",
       "      <th>the</th>\n",
       "      <th>Science</th>\n",
       "      <th>is</th>\n",
       "      <th>key</th>\n",
       "      <th>Machine</th>\n",
       "      <th>science</th>\n",
       "      <th>best</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060206</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    century      Data        of       for      21st       job  learning  \\\n",
       "0  0.030103  0.030103  0.030103  0.000000  0.030103  0.030103  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.037629  0.000000  0.000000  0.037629   \n",
       "\n",
       "        the   Science        is       key   Machine   science      best  \\\n",
       "0  0.060206  0.030103  0.030103  0.000000  0.000000  0.000000  0.030103   \n",
       "1  0.037629  0.000000  0.037629  0.037629  0.037629  0.037629  0.000000   \n",
       "\n",
       "       data  \n",
       "0  0.000000  \n",
       "1  0.037629  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute Term Frequency(TF) - Inverse Document Frequency(IDF)\n",
    "\n",
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return(tfidf)\n",
    "\n",
    "#running our two sentences through the IDF:\n",
    "idfFirst = computeTFIDF(tfFirst, idfs)\n",
    "idfSecond = computeTFIDF(tfSecond, idfs)\n",
    "\n",
    "#putting it in a dataframe\n",
    "pd.DataFrame([idfFirst, idfSecond])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e-6YcxsJHzsG",
    "outputId": "8c51d011-4a07-4231-df8c-35bf1a90ca97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.34211869506421816\n",
      "  (0, 0)\t0.34211869506421816\n",
      "  (0, 9)\t0.34211869506421816\n",
      "  (0, 5)\t0.34211869506421816\n",
      "  (0, 11)\t0.34211869506421816\n",
      "  (0, 12)\t0.48684053853849035\n",
      "  (0, 4)\t0.24342026926924518\n",
      "  (0, 10)\t0.24342026926924518\n",
      "  (0, 2)\t0.24342026926924518\n",
      "  (1, 3)\t0.40740123733358447\n",
      "  (1, 6)\t0.40740123733358447\n",
      "  (1, 7)\t0.40740123733358447\n",
      "  (1, 8)\t0.40740123733358447\n",
      "  (1, 12)\t0.28986933576883284\n",
      "  (1, 4)\t0.28986933576883284\n",
      "  (1, 10)\t0.28986933576883284\n",
      "  (1, 2)\t0.28986933576883284\n"
     ]
    }
   ],
   "source": [
    "#Compute TF-IDF\n",
    "\n",
    "#first step is to import the library\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#for the sentence, make sure all words are lowercase or you will run #into error. for simplicity, I just made the same sentence all #lowercase\n",
    "firstV= \"Data Science is the sexiest job of the 21st century\"\n",
    "secondV= \"machine learning is the key for data science\"\n",
    "\n",
    "#calling the TfidfVectorizer\n",
    "vectorize= TfidfVectorizer()\n",
    "\n",
    "#fitting the model and passing our sentences right away:\n",
    "response= vectorize.fit_transform([firstV, secondV])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Expt.No.7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
